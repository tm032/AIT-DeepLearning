{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1xCp6n57BB2"
      },
      "source": [
        "# RAG + LLM Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmI3jBxK68-G"
      },
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3STgq_s6_mV"
      },
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documentsor,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9nwufKe08D3e"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate > null\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO25-k4B7VRE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG2UTBG_7brU",
        "outputId": "9f158780-c30a-4fe0-d3ce-93aa93542354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# you need to define your private User Access Token from Huggingface\n",
        "# to be able to access models with accepted licence\n",
        "HUGGINGFACE_UAT=\"\"\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0d4rBXF-_h"
      },
      "source": [
        "## 1. Topic: Latest scientific papers about Cryptography and Security from arxiv.org.  (20 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0NGQcBGGnNg",
        "outputId": "87fea5d6-e458-4e39-96b1-46b6caf73f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-08 18:09:40--  https://arxiv.org/pdf/2405.01411\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25297840 (24M) [application/pdf]\n",
            "Saving to: ‘document1.pdf’\n",
            "\n",
            "\rdocument1.pdf         0%[                    ]       0  --.-KB/s               \rdocument1.pdf       100%[===================>]  24.12M   152MB/s    in 0.2s    \n",
            "\n",
            "2024-05-08 18:09:40 (152 MB/s) - ‘document1.pdf’ saved [25297840/25297840]\n",
            "\n",
            "--2024-05-08 18:09:40--  https://arxiv.org/pdf/2405.01080\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12161435 (12M) [application/pdf]\n",
            "Saving to: ‘document2.pdf’\n",
            "\n",
            "document2.pdf       100%[===================>]  11.60M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-08 18:09:41 (108 MB/s) - ‘document2.pdf’ saved [12161435/12161435]\n",
            "\n",
            "--2024-05-08 18:09:41--  https://arxiv.org/pdf/2405.01509\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203131 (198K) [application/pdf]\n",
            "Saving to: ‘document3.pdf’\n",
            "\n",
            "document3.pdf       100%[===================>] 198.37K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-08 18:09:41 (7.84 MB/s) - ‘document3.pdf’ saved [203131/203131]\n",
            "\n",
            "--2024-05-08 18:09:41--  https://arxiv.org/pdf/2405.00865\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1814225 (1.7M) [application/pdf]\n",
            "Saving to: ‘document4.pdf’\n",
            "\n",
            "document4.pdf       100%[===================>]   1.73M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-08 18:09:41 (27.9 MB/s) - ‘document4.pdf’ saved [1814225/1814225]\n",
            "\n",
            "--2024-05-08 18:09:41--  https://arxiv.org/pdf/2405.01495\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3481506 (3.3M) [application/pdf]\n",
            "Saving to: ‘document5.pdf’\n",
            "\n",
            "document5.pdf       100%[===================>]   3.32M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-05-08 18:09:41 (49.5 MB/s) - ‘document5.pdf’ saved [3481506/3481506]\n",
            "\n",
            "--2024-05-08 18:09:41--  https://arxiv.org/pdf/2405.01460\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4355225 (4.2M) [application/pdf]\n",
            "Saving to: ‘document6.pdf’\n",
            "\n",
            "document6.pdf       100%[===================>]   4.15M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-05-08 18:09:41 (55.6 MB/s) - ‘document6.pdf’ saved [4355225/4355225]\n",
            "\n",
            "--2024-05-08 18:09:41--  https://arxiv.org/pdf/2405.01030\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 630920 (616K) [application/pdf]\n",
            "Saving to: ‘document7.pdf’\n",
            "\n",
            "document7.pdf       100%[===================>] 616.13K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-08 18:09:41 (13.6 MB/s) - ‘document7.pdf’ saved [630920/630920]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# All papers were published after April 28th, 2024\n",
        "# \"IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party Apps\"\n",
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01411\"\n",
        "\n",
        "# \"KDPrint: Passive Authentication using Keystroke Dynamics-to-Image Encoding via Standardization\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01080\"\n",
        "\n",
        "# \"Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01509\"\n",
        "\n",
        "# \"Hiding Sensitive Information Using PDF Steganography\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00865\"\n",
        "\n",
        "# \"Error Correction Capabilities of Non-Linear Cryptographic Hash Functions\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01495\"\n",
        "\n",
        "# \"Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders\"\n",
        "!wget -O document6.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01460\"\n",
        "\n",
        "# \"Towards Trust Proof for Secure Confidential Virtual Machines\"\n",
        "!wget -O document7.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01030\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzXGfbxjHoHg"
      },
      "source": [
        "##2. Create three relevant prompts to the dataset, and one irrelevant prompt.  (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "mZdcTZXDHrYF"
      },
      "outputs": [],
      "source": [
        "question_1 = \"Explain what IDPFilter is, as proposed in a paper published in 2024?\"\n",
        "question_2 = \"How can one hide malwares in a PDF document using PDF steganography?\"\n",
        "question_3 = \"What is the benefit of encoding keystokes as images for training keystroke authentication mechanism, as proposed in a paper published in 2024?\"\n",
        "irrelevant_question = \"What is AI?\" # This is an irrelevant question, as the chosen topic is Cryptography and Security."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tVSCewNKo6B"
      },
      "source": [
        "##3. Load an LLM with at least 5B parameters. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "f838becfb72a4e6290d74455f13ecebe",
            "6b26877750e547f2b35363d87c44fb99",
            "5aba795186ad449e9e3d04e1a3227eb2",
            "7caac0d710364d6fa981d95f722901b2",
            "fd205a39c3734149a66a803df3dd6922",
            "f400188b35aa48e6950d4b548552d420",
            "d3140ef8c9bd4158875fae92ffe7043b",
            "c6808c65bd8c41189cf129733f3680e6",
            "c80dde8b6d5e4fcdaab77fdb1b8abec2",
            "0e72c5e749dd4d389047b27ef82915b4",
            "7d300024484b42a6b43626a53577c27f"
          ]
        },
        "id": "fvdnEdI_-U4g",
        "outputId": "8ff3dbd4-a3f2-4e7b-bf1a-59d57bfa90aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f838becfb72a4e6290d74455f13ecebe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8B language model from Meta AI\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # For RAG we would like to have determenistic answers\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm_original = HuggingFacePipeline(pipeline=pipe) # Original model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdOk1zM4Lw1_"
      },
      "source": [
        "##4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset.  (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mL86ltvaL0VI"
      },
      "outputs": [],
      "source": [
        "template_orginal = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_original = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template_orginal,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZdYKEhGL86Q",
        "outputId": "a3b68591-dd55-4d57-bad8-1ba85125d8f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  Explain what IDPFilter is, as proposed in\n",
            "a paper published in 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but\n",
            "since the paper you're referring to was published in 2024, it's likely that I don't have access to\n",
            "that information yet. As of my training data cutoff in 2022, there may not be any papers published\n",
            "in 2024.  However, if you can provide me with more context or details about the paper, such as its\n",
            "title, authors, or a brief summary, I'd be happy to try and help you understand what IDPFilter is.\n",
            "Alternatively, you can also try searching for the paper online or checking academic databases like\n",
            "Google Scholar or ResearchGate to see if they have any information on the topic.\n"
          ]
        }
      ],
      "source": [
        "result = llm_original(prompt_original.format(text=question_1))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkKAmV1QMFUN",
        "outputId": "a67ef54f-d363-4d1b-d17e-da0537f31b22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  How can one hide malwares in a PDF\n",
            "document using PDF steganography?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I cannot\n",
            "provide information or guidance on illegal activities, including hiding malware in a PDF document.\n",
            "Is there anything else I can help you with?\n"
          ]
        }
      ],
      "source": [
        "result = llm_original(prompt_original.format(text=question_2))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWibr-HVMFcJ",
        "outputId": "b3023619-cedd-4fb0-ec50-37ddc50ea06a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What is the benefit of encoding keystokes\n",
            "as images for training keystroke authentication mechanism, as proposed in a paper published in\n",
            "2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but since I'm an AI\n",
            "trained until 2021, I don't have information about papers published in 2024. Additionally, I\n",
            "couldn't find any specific information on encoding keystrokes as images for training keystroke\n",
            "authentication mechanisms.  However, I can provide some general insights on keystroke authentication\n",
            "and image-based approaches:  Keystroke authentication is a biometric method that analyzes the unique\n",
            "patterns of typing behavior, such as timing, pressure, and movement, to verify an individual's\n",
            "identity. Encoding keystrokes as images could potentially be used to create a visual representation\n",
            "of the typing pattern, which might help improve the accuracy or robustness of the authentication\n",
            "system.  Some potential benefits of using image-based representations of keystrokes include:  1.\n",
            "**Improved feature extraction**: Images can capture more detailed information about the typing\n",
            "pattern, such as subtle variations in finger movement, pressure, and speed. 2. **Robustness against\n",
            "noise and variability**: Image processing techniques can help filter out noise and normalize the\n",
            "data, making it less susceptible to environmental factors like keyboard layout, lighting conditions,\n",
            "or typing style changes. 3. **Enhanced security**: By converting keystrokes into images, you may be\n",
            "able to leverage computer vision techniques to detect and prevent tampering attempts, such as replay\n",
            "attacks or spoofing. 4. **Increased usability**: An image-based approach might make it easier for\n",
            "users to enroll and authenticate, as they would only need to type their password once to generate\n",
            "the image, rather than repeatedly typing it during the enrollment process.  Please note that these\n",
            "are speculative benefits, and I couldn't find any concrete evidence or research supporting the use\n",
            "of image-based keystroke authentication. If you're interested in learning more about this topic, I\n",
            "recommend searching for recent research papers or consulting with experts in the field.\n"
          ]
        }
      ],
      "source": [
        "result = llm_original(prompt_original.format(text=question_3))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNyU5JffbfQp",
        "outputId": "c4045eb3-effb-4eba-b0c2-cefd8a13a010"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What is\n",
            "AI?<|eot_id|><|start_header_id|>assistant<|end_header_id|> Artificial Intelligence (AI) refers to\n",
            "the development of computer systems that can perform tasks that typically require human\n",
            "intelligence, such as:  1. **Learning**: AI systems can learn from data and improve their\n",
            "performance over time. 2. **Reasoning**: AI systems can draw conclusions and make decisions based on\n",
            "given information. 3. **Problem-solving**: AI systems can identify problems and find solutions. 4.\n",
            "**Perception**: AI systems can interpret and understand data from sensors, images, or other sources.\n",
            "There are many types of AI, including:  1. **Narrow or Weak AI**: Designed to perform a specific\n",
            "task, like playing chess or recognizing faces. 2. **General or Strong AI**: A hypothetical AI that\n",
            "can perform any intellectual task that a human can. 3. **Superintelligence**: An AI that is\n",
            "significantly more intelligent than the best human minds.  Some examples of AI applications include:\n",
            "1. Virtual assistants: Siri, Alexa, Google Assistant 2. Image recognition: Facial recognition,\n",
            "object detection in self-driving cars 3. Natural Language Processing (NLP): Chatbots, language\n",
            "translation, sentiment analysis 4. Robotics: Autonomous vehicles, robotic arms, drones 5. Expert\n",
            "systems: Medical diagnosis, financial forecasting, customer service  The benefits of AI include:  1.\n",
            "**Increased efficiency**: Automating repetitive tasks, freeing up humans for more complex work. 2.\n",
            "**Improved accuracy**: Reducing errors and improving decision-making through data-driven insights.\n",
            "3. **Enhanced innovation**: Allowing humans to focus on creative tasks while AI handles routine\n",
            "tasks.  However, there are also concerns about the potential risks and challenges associated with\n",
            "AI, such as:  1. **Job displacement**: Automation replacing human jobs. 2. **Bias and ethics**: AI\n",
            "systems reflecting and amplifying existing biases. 3. **Security threats**: Cybersecurity risks\n",
            "associated with AI-powered attacks.  Overall, AI has the potential to revolutionize various\n",
            "industries and aspects of our lives, but it's essential to consider its limitations, challenges, and\n",
            "ethical implications as we continue to develop and integrate AI into our society.\n"
          ]
        }
      ],
      "source": [
        "result = llm_original(prompt_original.format(text=irrelevant_question))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUhPrFWVMNe-"
      },
      "source": [
        "##5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "JejmneOZ-coP"
      },
      "outputs": [],
      "source": [
        "# Referenced https://community.openai.com/t/out-of-context-questions-in-retrieval-augmented-generation/434871 to prevent the LLM to produce answers to the irrelevant question.\n",
        "template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to answer the question at the end. Do not make up an answer. If a response cannot be formed strictly using the context, just say I don't have enough relevant information to answer the question.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\",\"text\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7l19PbbbMFAU"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "import os\n",
        "loaders = [UnstructuredPDFLoader(fn) for fn in [\"/content/document1.pdf\",\n",
        "                                                \"/content/document2.pdf\",\n",
        "                                                \"/content/document3.pdf\",\n",
        "                                                \"/content/document4.pdf\",\n",
        "                                                \"/content/document5.pdf\",\n",
        "                                                \"/content/document6.pdf\",\n",
        "                                                \"/content/document7.pdf\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVqwE01fMOFo",
        "outputId": "1c5ee492-6959-4743-bbe5-aa38b9b7ed6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading raw document.../content/document1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document2.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document3.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document4.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document5.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document6.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document7.pdf\n",
            "Splitting text...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "542"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunked_pdf_doc = []\n",
        "\n",
        "for loader in loaders:\n",
        "    print(\"Loading raw document...\" + loader.file_path)\n",
        "    pdf_doc = loader.load()\n",
        "    updated_pdf_doc = filter_complex_metadata(pdf_doc)\n",
        "    print(\"Splitting text...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "    documents = text_splitter.split_documents(updated_pdf_doc)\n",
        "    chunked_pdf_doc.extend(documents)\n",
        "\n",
        "len(chunked_pdf_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k82LG_uHMStd",
        "outputId": "5ccc3807-522b-4790-e0eb-a1643b51964a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11.8 s, sys: 282 ms, total: 12.1 s\n",
            "Wall time: 13.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PESXQsrrMx6M"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe) # Loading seven papers\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF4TJl6UNrjY"
      },
      "source": [
        "##6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeZEw0G6Nr_w",
        "outputId": "772d112e-6cab-4716-9c32-5a7ad091e5cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not make up an answer. If a response cannot be formed strictly using the\n",
            "context, just say I don't have enough relevant information to answer the question.  5.5 IDPFilter:\n",
            "mitigation scheme  Our IDP mitigation scheme comprises three distinct interactions with regard to a\n",
            "given pair (app, user) during IDPFilter’s operation (see Fig. 8):  1. Filter setup. Initially, the\n",
            "IDP service provides a standard API to all inter- ested third-party applications, encompassing\n",
            "various filtering schemes under  IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party\n",
            "Apps  Fig. 6. Overview of the IDPFilter API framework  different constraints. Third-party\n",
            "application developers can select and inte- grate the services that best align with their\n",
            "requirements. The Filter Setup can be done at any time the users use the service.  23 third parties\n",
            "are stakeholders outside of users, app provider, and IDPFilter  23  24  Shuaishuai Liu and Gergely\n",
            "Bicz´ok  The reasons why we chose to focus on text filtering over other forms of data  are the\n",
            "following:  1. Ubiquity and relevance. Textual data is the most common type of data people share and\n",
            "interact with online. From social media posts to emails to articles, text is everywhere. Hence, text\n",
            "is indeed a relevant data modality for our proof-of-concept (PoC).  2. Simplicity and speed. Text\n",
            "processing is generally simpler and faster than processing images or video. It requires less\n",
            "computational power and storage, which has enabled us to focus on demonstrating IDPFilter in action\n",
            "and avoid making processing cost and efficiency our primary concern.  IDPFilter can be deployed as a\n",
            "web service and hosted by us or any trusted providers. Following the design principles P1-P5 (see\n",
            "Section 5.3), we provide a series of interfaces available to any third-party application; this\n",
            "ensures that IDP protection is available and hassle-free so that app developers can use it easily as\n",
            "opposed to coding their own IDP protection functionality. The app needs to call the API to realize\n",
            "the function of IDP text filtering. If app developers integrate the IDPFilter API into their app,\n",
            "then users can both i) customize and ii) enjoy IDPFilter’s features. From the users’ perspective,\n",
            "APIs are presented as a (graphical) user interface integrated into the app. Therefore, users do not\n",
            "have to have technical knowledge in order to be protected by IDPFilter.  6.1 IDPTextFilter:\n",
            "structure  3. Data Flow. Upon completing the first two stages, users share information through the\n",
            "third-party application, which utilizes the IDPFilter service. Based on the granted permissions and\n",
            "the chosen filtering scheme, the infor- mation is filtered before being shared with other users or\n",
            "third parties23 as in Figure 5.  6 Proof-of-Concept: IDPTextFilter API  Users share multi-modal data\n",
            "via third-party applications, including text, voice, images, and videos. Technologies for\n",
            "image/voice/video recognition exist that could enable us to flag and reduce IDP-related leaks in all\n",
            "information modalities above. Nevertheless, in this section, we focus on basic textual data to\n",
            "illustrate the mechanism of mitigating IDP issues.  23 third parties are stakeholders outside of\n",
            "users, app provider, and IDPFilter  23  24  Shuaishuai Liu and Gergely Bicz´ok  The reasons why we\n",
            "chose to focus on text filtering over other forms of data  are the\n",
            "following:<|eot_id|><|start_header_id|>user<|end_header_id|>  Explain what IDPFilter is, as proposed\n",
            "in a paper published in 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> According to\n",
            "the provided context, IDPFilter is a mitigation scheme designed to address Interdependent Privacy\n",
            "Issues (IDP) in third-party apps. It consists of three interactions between the IDP service and a\n",
            "given pair of an app and a user during its operation. The goal of IDPFilter is to provide a\n",
            "standardized API framework for third-party applications to integrate and utilize IDP protection\n",
            "features, ensuring that users' personal data is protected from unauthorized sharing or leakage.\n",
            "Specifically, IDPFilter focuses on text filtering, aiming to simplify and accelerate the process\n",
            "while leveraging the ubiquity and relevance of textual data.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(question_1)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jactFI9DRTJW",
        "outputId": "dcadff30-10ee-46c8-b78a-bbcf9cbb8378"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not make up an answer. If a response cannot be formed strictly using the\n",
            "context, just say I don't have enough relevant information to answer the question.  document. With\n",
            "our steganography Python script, that is done with the following command: $ python3 full pdf steg.py\n",
            "stat cover.pdf Which gives the results shown:  Our malware sample has a size of 335,872 bytes, which\n",
            "is clearly smaller than the 464,007 byte carrying capacity of the cover PDF. Thus, we can continue\n",
            "on with the process. The command to embed the hidden data within the PDF is the following: $ python3\n",
            "full pdf steg.py embed cover.pdf steg.pdf trojan-petya The results of this command are shown below:\n",
            "After the steganography algorithm finishes processing the PDF file, the changes made in the document\n",
            "are almost imperceptible. To show an example, a page before (left) and after (right) data hiding is\n",
            "shown below:  7  Cover PDF – Adobe PDF Standard Specification, Version 1.7 – Source:\n",
            "https://opensource.adobe.com/ dc-acrobat-sdk-docs/pdfstandards/PDF32000_2008.pdf Malware Sample –\n",
            "Petya Ransomware Trojan (Windows) – Source: https://github.com/ytisf/theZoo/\n",
            "tree/master/malware/Binaries/Trojan.Ransom.Petya  Information on the malware sample file is shown in\n",
            "the below image:  Before we can operate on the PDF document, we must decompress the streams using\n",
            "one of the methods  discussed in Section 2.4: $ qpdf PDF32000 2008.pdf --stream-data=uncompress\n",
            "cover.pdf This yields the file cover.pdf, which has all object stream data in cleartext.  Before\n",
            "attempting to embed the data, it is important to check the actual capacity of the cover PDF\n",
            "document. With our steganography Python script, that is done with the following command: $ python3\n",
            "full pdf steg.py stat cover.pdf Which gives the results shown:  A variety of methods have been\n",
            "developed in order to hide data inside of PDF documents. Three of the major categories of PDF\n",
            "steganography are described in the following sections. This research will primarily focus on using\n",
            "“Operator Value” of the PDF documents for information hiding via steganography. Data Outside of\n",
            "View. One of the simplest forms of PDF file steganography is placing data in areas of the file in\n",
            "which it will not be rendered when opened by a PDF reader. This could be as simple as placing hidden\n",
            "data within a PDF stream comment or after the %%EOF line of the file. More complex strategies\n",
            "include placing hidden data inside an object that is not linked to the main object tree within the\n",
            "PDF – for example, a page object that is not linked into the document’s page catalogue [4]. The main\n",
            "drawback to this steganography method is that it can be easily detected and defeated. Between-\n",
            "Character, Between-Word, Between-Line. Similar to other forms of text-file steganography, whitespace\n",
            "is a vector for  Net Increase in PDF size (compressed streams) = 418,039 bytes  Rate (steg message\n",
            "capacity / compressed cover size) = 2.06%  Rate (steg message size / decompressed size increase) =\n",
            "23.25%  Rate (steg message size / compressed size increase) = 80.34%  Rate (decompressed size\n",
            "increase / decompressed cover size) = 4.68%  Rate (compressed size increase / compressed cover size)\n",
            "= 1.86%  Overall, this sample run of the PDF steganography method successfully embedded a 335 KB\n",
            "malware sample into a 22.5 MB PDF file. The resulting PDF file has a size of 22.9 MB, which is an\n",
            "net size increase of less than 2%. The steganographic encoding very efficiently uses the additional\n",
            "size that it adds to the PDF file.  6 Conclusion<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "How can one hide malwares in a PDF document using PDF\n",
            "steganography?<|eot_id|><|start_header_id|>assistant<|end_header_id|> Based on the provided context,\n",
            "here's how one can hide malware in a PDF document using PDF steganography:  1. Choose a cover PDF\n",
            "document: Select a PDF file that you want to use as a carrier for your malware. 2. Check the\n",
            "capacity of the cover PDF: Use a Python script to determine the carrying capacity of the cover PDF,\n",
            "which is the maximum amount of data that can be embedded without increasing the file size\n",
            "significantly. 3. Decompress any object streams: Use tools like `qpdf` to decompress any object\n",
            "streams in the cover PDF, making it easier to work with the file. 4. Embed the malware: Use a Python\n",
            "script to embed the malware into the cover PDF. In this case, the script is called `full pdf\n",
            "steg.py`, and it takes three arguments: the name of the cover PDF, the output PDF file, and the\n",
            "malware file. 5. Run the embedding command: Execute the command `$ python3 full pdf steg.py embed\n",
            "cover.pdf steg.pdf trojan-petya` to embed the malware into the cover PDF. This will create a new PDF\n",
            "file named `steg.pdf` that contains the malware. 6. Verify the result: The resulting PDF file should\n",
            "have a size close to the original cover PDF, but with the added malware embedded. The changes made\n",
            "to the document should be almost imperceptible.  Note that this is just one way to hide malware in a\n",
            "PDF document using PDF steganography, and there may be other techniques and tools available\n",
            "depending on the specific requirements and constraints.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(question_2)\n",
        "print(fill(result['result'].strip(), width=100))\n",
        "\n",
        "# It is interesting that the Llama with RAG is happy to give answers on how to hide malwares in PDF,\n",
        "#  while the original Llama considered answering the question would promote illegal activities and rejected giving any pieces of information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-kuIiWuOKw4",
        "outputId": "3c86a3ed-8745-4b1d-d1e5-672f0c42410d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not make up an answer. If a response cannot be formed strictly using the\n",
            "context, just say I don't have enough relevant information to answer the question.  Figure 1: The\n",
            "overall user authentication process of the proposed system.  3  KDPrint: Passive Authentication\n",
            "using Keystroke Dynamics-to-Image Encoding via Standardization  3.1 System overview  10  KDPrint:\n",
            "Passive Authentication using Keystroke Dynamics-to-Image Encoding via Standardization  6 Conclusion\n",
            "5  (1)  KDPrint: Passive Authentication using Keystroke Dynamics-to-Image Encoding via\n",
            "Standardization  indicates the size of the marker (*) at that position. Color represents the color\n",
            "of the marker, with three channels for red, green, and blue. Lastly, Opacity indicates the\n",
            "transparency of the marker. Due to the standardization of data, in the ideal user input, all\n",
            "keystrokes converge to the origin, resulting in a stabilizing effect. Consequently, the image of a\n",
            "legitimate user tends to exhibit greater stability compared to that of an impostor, as illustrated\n",
            "in Fig. 5.  Figure 3: The example of feature-engineered dataset table and image generated using our\n",
            "proposed method.  Figure 4: The keystroke dynamics to image encoding result with different scale.\n",
            "The proposed method for image generation involves representing all keystroke information on a single\n",
            "keypad button. The essential data types required for generating these images encompass Position,\n",
            "Marker Size, Color, and Opacity. Position signifies the touch location within the key and is\n",
            "represented using standardized (x, y) data. Marker size  5  (1)  KDPrint: Passive Authentication\n",
            "using Keystroke Dynamics-to-Image Encoding via\n",
            "Standardization<|eot_id|><|start_header_id|>user<|end_header_id|>  What is the benefit of encoding\n",
            "keystokes as images for training keystroke authentication mechanism, as proposed in a paper\n",
            "published in 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> According to the provided\n",
            "text, one benefit of encoding keystrokes as images is that it allows for a \"stabilizing effect\" due\n",
            "to the standardization of data. This means that the images of legitimate users tend to exhibit\n",
            "greater stability compared to those of impostors.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(question_3)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRmu3y28OLPg",
        "outputId": "19af43fa-5171-4101-ae4c-1ccecda779dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not make up an answer. If a response cannot be formed strictly using the\n",
            "context, just say I don't have enough relevant information to answer the question.\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>  What is\n",
            "AI?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I don't have enough relevant information\n",
            "to answer the question.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(irrelevant_question)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXXK5tAbafyf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e72c5e749dd4d389047b27ef82915b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aba795186ad449e9e3d04e1a3227eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6808c65bd8c41189cf129733f3680e6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c80dde8b6d5e4fcdaab77fdb1b8abec2",
            "value": 4
          }
        },
        "6b26877750e547f2b35363d87c44fb99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f400188b35aa48e6950d4b548552d420",
            "placeholder": "​",
            "style": "IPY_MODEL_d3140ef8c9bd4158875fae92ffe7043b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7caac0d710364d6fa981d95f722901b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e72c5e749dd4d389047b27ef82915b4",
            "placeholder": "​",
            "style": "IPY_MODEL_7d300024484b42a6b43626a53577c27f",
            "value": " 4/4 [01:20&lt;00:00, 17.28s/it]"
          }
        },
        "7d300024484b42a6b43626a53577c27f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6808c65bd8c41189cf129733f3680e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80dde8b6d5e4fcdaab77fdb1b8abec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3140ef8c9bd4158875fae92ffe7043b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f400188b35aa48e6950d4b548552d420": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f838becfb72a4e6290d74455f13ecebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b26877750e547f2b35363d87c44fb99",
              "IPY_MODEL_5aba795186ad449e9e3d04e1a3227eb2",
              "IPY_MODEL_7caac0d710364d6fa981d95f722901b2"
            ],
            "layout": "IPY_MODEL_fd205a39c3734149a66a803df3dd6922"
          }
        },
        "fd205a39c3734149a66a803df3dd6922": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
