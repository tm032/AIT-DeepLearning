{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1xCp6n57BB2"
      },
      "source": [
        "# RAG + LLM Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmI3jBxK68-G"
      },
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3STgq_s6_mV"
      },
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documentsor,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9nwufKe08D3e"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate > null\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO25-k4B7VRE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG2UTBG_7brU",
        "outputId": "21540539-c2af-482b-f1a4-0f75edab7888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# you need to define your private User Access Token from Huggingface\n",
        "# to be able to access models with accepted licence\n",
        "HUGGINGFACE_UAT=\"\" # Hiding the UAT\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0d4rBXF-_h"
      },
      "source": [
        "## 1. Topic: Latest scientific papers about security and cryptography from arxiv.org.  (20 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0NGQcBGGnNg",
        "outputId": "bc8b55cf-c116-4828-84f4-09c88898513d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-06 00:41:50--  https://arxiv.org/pdf/2405.01411\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25297840 (24M) [application/pdf]\n",
            "Saving to: ‘document1.pdf’\n",
            "\n",
            "\rdocument1.pdf         0%[                    ]       0  --.-KB/s               \rdocument1.pdf       100%[===================>]  24.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-06 00:41:50 (188 MB/s) - ‘document1.pdf’ saved [25297840/25297840]\n",
            "\n",
            "--2024-05-06 00:41:50--  https://arxiv.org/pdf/2405.01080\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12161435 (12M) [application/pdf]\n",
            "Saving to: ‘document2.pdf’\n",
            "\n",
            "document2.pdf       100%[===================>]  11.60M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-05-06 00:41:50 (159 MB/s) - ‘document2.pdf’ saved [12161435/12161435]\n",
            "\n",
            "--2024-05-06 00:41:51--  https://arxiv.org/pdf/2405.01509\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203131 (198K) [application/pdf]\n",
            "Saving to: ‘document3.pdf’\n",
            "\n",
            "document3.pdf       100%[===================>] 198.37K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-06 00:41:51 (12.5 MB/s) - ‘document3.pdf’ saved [203131/203131]\n",
            "\n",
            "--2024-05-06 00:41:51--  https://arxiv.org/pdf/2405.00865\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1814225 (1.7M) [application/pdf]\n",
            "Saving to: ‘document4.pdf’\n",
            "\n",
            "document4.pdf       100%[===================>]   1.73M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-06 00:41:51 (50.7 MB/s) - ‘document4.pdf’ saved [1814225/1814225]\n",
            "\n",
            "--2024-05-06 00:41:51--  https://arxiv.org/pdf/2405.01495\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3481506 (3.3M) [application/pdf]\n",
            "Saving to: ‘document5.pdf’\n",
            "\n",
            "document5.pdf       100%[===================>]   3.32M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-06 00:41:51 (76.9 MB/s) - ‘document5.pdf’ saved [3481506/3481506]\n",
            "\n",
            "--2024-05-06 00:41:51--  https://arxiv.org/pdf/2405.01460\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4355568 (4.2M) [application/pdf]\n",
            "Saving to: ‘document6.pdf’\n",
            "\n",
            "document6.pdf       100%[===================>]   4.15M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-05-06 00:41:51 (77.9 MB/s) - ‘document6.pdf’ saved [4355568/4355568]\n",
            "\n",
            "--2024-05-06 00:41:51--  https://arxiv.org/pdf/2405.01030\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 630920 (616K) [application/pdf]\n",
            "Saving to: ‘document7.pdf’\n",
            "\n",
            "document7.pdf       100%[===================>] 616.13K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-06 00:41:51 (25.6 MB/s) - ‘document7.pdf’ saved [630920/630920]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# All papers were published after April 28th, 2024\n",
        "# \"IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party Apps\"\n",
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01411\"\n",
        "\n",
        "# \"KDPrint: Passive Authentication using Keystroke Dynamics-to-Image Encoding via Standardization\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01080\"\n",
        "\n",
        "# \"Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01509\"\n",
        "\n",
        "# \"Hiding Sensitive Information Using PDF Steganography\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00865\"\n",
        "\n",
        "# \"Error Correction Capabilities of Non-Linear Cryptographic Hash Functions\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01495\"\n",
        "\n",
        "# \"Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders\"\n",
        "!wget -O document6.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01460\"\n",
        "\n",
        "# \"Towards Trust Proof for Secure Confidential Virtual Machines\"\n",
        "!wget -O document7.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01030\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzXGfbxjHoHg"
      },
      "source": [
        "##2. Create three relevant prompts to the dataset, and one irrelevant prompt.  (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "mZdcTZXDHrYF"
      },
      "outputs": [],
      "source": [
        "question_1 = \"Explain what IDPFilter is, as proposed in a paper published in 2024?\"\n",
        "question_2 = \"How can one cryptographically hide malwares in a PDF document?\"\n",
        "question_3 = \"What is the benefit of encoding keystokes as images for training keystroke authentication mechanism, as proposed in a paper published in 2024?\"\n",
        "irrelevant_question = \"Where is the name of the U.S. President?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tVSCewNKo6B"
      },
      "source": [
        "##3. Load an LLM with at least 5B parameters. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "5d8eeac44145445aadaeb18c817816e2",
            "a4def2765a4444f5af7e6bde16afd182",
            "717fd309e11d4cabbf5bed389caebc76",
            "d6dd0b5757014f88ac54061bb41f53ac",
            "30dff059815140e3afe64c656f450070",
            "c149c72550864e3c8151b7b77466e17b",
            "21734d4bb29d4ebd823561b9612d41ad",
            "f4c22ed6f46c4819be223a5843b93485",
            "206fd266fbf546439d703c34899f9035",
            "5faea4b926f2417f9652662de59e4cbb",
            "ed0c4a982cf54ac3adc2abf9efb689e5"
          ]
        },
        "id": "fvdnEdI_-U4g",
        "outputId": "3bf58ab5-a91f-4d5e-cb66-e5ff8c5d413e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d8eeac44145445aadaeb18c817816e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8B language model from Meta AI\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0 # For RAG we would like to have determenistic answers\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm_original = HuggingFacePipeline(pipeline=pipe) # Original model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdOk1zM4Lw1_"
      },
      "source": [
        "##4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset.  (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "mL86ltvaL0VI"
      },
      "outputs": [],
      "source": [
        "template_orginal = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_original = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template_orginal,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZdYKEhGL86Q",
        "outputId": "96d6c78f-a272-484c-db10-2d9f0a9b7597"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  Explain what IDPFilter is, as proposed in\n",
            "a paper published in 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but\n",
            "since the paper you're referring to was published in 2024, it's likely that I don't have access to\n",
            "that information yet. As of my training data cutoff in 2022, there may not be any papers published\n",
            "in 2024.  However, if you can provide me with more context or details about the paper, such as its\n",
            "title, authors, or a brief summary, I'd be happy to try and help you understand what IDPFilter is.\n",
            "Alternatively, you can also try searching for the paper online or checking academic databases like\n",
            "Google Scholar or ResearchGate to see if they have any information on the topic.\n"
          ]
        }
      ],
      "source": [
        "result = llm_original(prompt_original.format(text=question_1))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkKAmV1QMFUN",
        "outputId": "7d66a4bc-5d42-4417-a0d3-749b1d7e6dd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  How can one cryptographically hide\n",
            "malwares in a PDF document?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I cannot provide\n",
            "information or guidance on illegal or harmful activities, including hiding malware in a PDF\n",
            "document. Can I help you with something else?\n"
          ]
        }
      ],
      "source": [
        "result = llm_original(prompt_original.format(text=question_2))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWibr-HVMFcJ",
        "outputId": "abec6a6f-8abe-49d2-f522-d516ff3a0a8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What is the benefit of encoding keystokes\n",
            "as images for training keystroke authentication mechanism, as proposed in a paper published in\n",
            "2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but since I'm an AI\n",
            "trained until 2021, I don't have information about papers published in 2024. Additionally, I\n",
            "couldn't find any specific information on encoding keystrokes as images for training keystroke\n",
            "authentication mechanisms.  However, I can provide some general insights on keystroke authentication\n",
            "and image-based approaches:  Keystroke authentication is a biometric method that analyzes the unique\n",
            "patterns of typing behavior, such as timing, pressure, and movement, to verify an individual's\n",
            "identity. Encoding keystrokes as images could potentially be used to create a visual representation\n",
            "of the typing pattern, which might help improve the accuracy or robustness of the authentication\n",
            "system.  Some potential benefits of using image-based representations of keystrokes include:  1.\n",
            "**Improved feature extraction**: Images can capture more detailed information about the typing\n",
            "pattern, such as subtle variations in finger movement, pressure, and speed. 2. **Robustness against\n",
            "noise and variability**: Image processing techniques can help filter out noise and normalize the\n",
            "data, making it less susceptible to environmental factors like keyboard layout, lighting conditions,\n",
            "or typing style changes. 3. **Enhanced security**: By converting keystrokes into images, you may be\n",
            "able to leverage computer vision techniques to detect and prevent tampering attempts, such as replay\n",
            "attacks or spoofing. 4. **Increased usability**: An image-based approach might make it easier for\n",
            "users to enroll and authenticate, as they would only need to type their password once to generate\n",
            "the image, rather than repeatedly typing it during the enrollment process.  Please note that these\n",
            "are speculative benefits, and I couldn't find any concrete evidence or research supporting the use\n",
            "of image-based keystroke authentication. If you're interested in learning more, I recommend\n",
            "searching for recent publications on keystroke authentication and exploring the work of researchers\n",
            "in this field.\n"
          ]
        }
      ],
      "source": [
        "result = llm_original(prompt_original.format(text=question_3))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUhPrFWVMNe-"
      },
      "source": [
        "##5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "JejmneOZ-coP"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If there is no relevant information in the context, just say I don't have enough information. Don't try to make up an answer.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\",\"text\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "7l19PbbbMFAU"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "import os\n",
        "loaders = [UnstructuredPDFLoader(fn) for fn in [\"/content/document1.pdf\",\n",
        "                                                \"/content/document2.pdf\",\n",
        "                                                \"/content/document3.pdf\",\n",
        "                                                \"/content/document4.pdf\",\n",
        "                                                \"/content/document5.pdf\",\n",
        "                                                \"/content/document6.pdf\",\n",
        "                                                \"/content/document7.pdf\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVqwE01fMOFo",
        "outputId": "49fc1483-5bbc-4a1a-cd4b-4124c669d834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading raw document.../content/document1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document2.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document3.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document4.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document5.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document6.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document7.pdf\n",
            "Splitting text...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "542"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunked_pdf_doc = []\n",
        "\n",
        "for loader in loaders:\n",
        "    print(\"Loading raw document...\" + loader.file_path)\n",
        "    pdf_doc = loader.load()\n",
        "    updated_pdf_doc = filter_complex_metadata(pdf_doc)\n",
        "    print(\"Splitting text...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "    documents = text_splitter.split_documents(updated_pdf_doc)\n",
        "    chunked_pdf_doc.extend(documents)\n",
        "\n",
        "len(chunked_pdf_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k82LG_uHMStd",
        "outputId": "99504e06-9d8f-48fc-9f00-bb7b8060e48d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11.7 s, sys: 160 ms, total: 11.9 s\n",
            "Wall time: 12.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "PESXQsrrMx6M"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe) # Trained with the five papers\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF4TJl6UNrjY"
      },
      "source": [
        "##6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeZEw0G6Nr_w",
        "outputId": "d62bd4a2-97ab-417e-ae48-a97ad8c6effb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If there is no relevant information in the\n",
            "context, just say I don't have enough information. Don't try to make up an answer.  5.5 IDPFilter:\n",
            "mitigation scheme  Our IDP mitigation scheme comprises three distinct interactions with regard to a\n",
            "given pair (app, user) during IDPFilter’s operation (see Fig. 8):  1. Filter setup. Initially, the\n",
            "IDP service provides a standard API to all inter- ested third-party applications, encompassing\n",
            "various filtering schemes under  IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party\n",
            "Apps  Fig. 6. Overview of the IDPFilter API framework  different constraints. Third-party\n",
            "application developers can select and inte- grate the services that best align with their\n",
            "requirements. The Filter Setup can be done at any time the users use the service.  23 third parties\n",
            "are stakeholders outside of users, app provider, and IDPFilter  23  24  Shuaishuai Liu and Gergely\n",
            "Bicz´ok  The reasons why we chose to focus on text filtering over other forms of data  are the\n",
            "following:  1. Ubiquity and relevance. Textual data is the most common type of data people share and\n",
            "interact with online. From social media posts to emails to articles, text is everywhere. Hence, text\n",
            "is indeed a relevant data modality for our proof-of-concept (PoC).  2. Simplicity and speed. Text\n",
            "processing is generally simpler and faster than processing images or video. It requires less\n",
            "computational power and storage, which has enabled us to focus on demonstrating IDPFilter in action\n",
            "and avoid making processing cost and efficiency our primary concern.  IDPFilter can be deployed as a\n",
            "web service and hosted by us or any trusted providers. Following the design principles P1-P5 (see\n",
            "Section 5.3), we provide a series of interfaces available to any third-party application; this\n",
            "ensures that IDP protection is available and hassle-free so that app developers can use it easily as\n",
            "opposed to coding their own IDP protection functionality. The app needs to call the API to realize\n",
            "the function of IDP text filtering. If app developers integrate the IDPFilter API into their app,\n",
            "then users can both i) customize and ii) enjoy IDPFilter’s features. From the users’ perspective,\n",
            "APIs are presented as a (graphical) user interface integrated into the app. Therefore, users do not\n",
            "have to have technical knowledge in order to be protected by IDPFilter.  6.1 IDPTextFilter:\n",
            "structure  3. Data Flow. Upon completing the first two stages, users share information through the\n",
            "third-party application, which utilizes the IDPFilter service. Based on the granted permissions and\n",
            "the chosen filtering scheme, the infor- mation is filtered before being shared with other users or\n",
            "third parties23 as in Figure 5.  6 Proof-of-Concept: IDPTextFilter API  Users share multi-modal data\n",
            "via third-party applications, including text, voice, images, and videos. Technologies for\n",
            "image/voice/video recognition exist that could enable us to flag and reduce IDP-related leaks in all\n",
            "information modalities above. Nevertheless, in this section, we focus on basic textual data to\n",
            "illustrate the mechanism of mitigating IDP issues.  23 third parties are stakeholders outside of\n",
            "users, app provider, and IDPFilter  23  24  Shuaishuai Liu and Gergely Bicz´ok  The reasons why we\n",
            "chose to focus on text filtering over other forms of data  are the\n",
            "following:<|eot_id|><|start_header_id|>user<|end_header_id|>  Explain what IDPFilter is, as proposed\n",
            "in a paper published in 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> According to\n",
            "the provided context, IDPFilter is a mitigation scheme designed to address Interdependent Privacy\n",
            "Issues (IDP) in third-party apps. It consists of three interactions between the IDP service and a\n",
            "given pair (app, user) during its operation. IDPFilter provides a standard API to third-party\n",
            "applications, allowing them to select and integrate filtering schemes that align with their\n",
            "requirements. The goal of IDPFilter is to protect users' privacy by filtering sensitive information\n",
            "before sharing it with other users or third parties.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(question_1)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jactFI9DRTJW",
        "outputId": "c045ab39-d51a-432b-ff65-9edd4f145f24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If there is no relevant information in the\n",
            "context, just say I don't have enough information. Don't try to make up an answer.  6 Conclusion  By\n",
            "utilizing an adaptable strategy of least-significant bit insertion on string representations of\n",
            "floating- point operands to perform imperceptible changes to PDF stream operators, this paper\n",
            "presents a novel PDF steganography method. Our method has a higher carrying capacity than previous\n",
            "PDF operator-based methods due to the use of all viable operators found within a PDF file. Our case\n",
            "study of embedding malware into a given PDF cover file justifies effectiveness of our proposed\n",
            "approach. Acknowledgments. This work was supported by US National Science Foundation under grant\n",
            "number CNS-1928349, CNS-2225424, and 2043022-DGE.  References  [1] Document management — Portable\n",
            "document format — Part 1: PDF 1.7. Standard. Geneva, CH:  International Organization for\n",
            "Standardization, July 2008.  [2] Document management — Portable document format — Part 2: PDF 2.0.\n",
            "Standard. Geneva, CH:  International Organization for Standardization, Dec. 2020.  Cover PDF – Adobe\n",
            "PDF Standard Specification, Version 1.7 – Source: https://opensource.adobe.com/ dc-acrobat-sdk-\n",
            "docs/pdfstandards/PDF32000_2008.pdf Malware Sample – Petya Ransomware Trojan (Windows) – Source:\n",
            "https://github.com/ytisf/theZoo/ tree/master/malware/Binaries/Trojan.Ransom.Petya  Information on\n",
            "the malware sample file is shown in the below image:  Before we can operate on the PDF document, we\n",
            "must decompress the streams using one of the methods  discussed in Section 2.4: $ qpdf PDF32000\n",
            "2008.pdf --stream-data=uncompress cover.pdf This yields the file cover.pdf, which has all object\n",
            "stream data in cleartext.  Before attempting to embed the data, it is important to check the actual\n",
            "capacity of the cover PDF  document. With our steganography Python script, that is done with the\n",
            "following command: $ python3 full pdf steg.py stat cover.pdf Which gives the results shown:  A\n",
            "variety of methods have been developed in order to hide data inside of PDF documents. Three of the\n",
            "major categories of PDF steganography are described in the following sections. This research will\n",
            "primarily focus on using “Operator Value” of the PDF documents for information hiding via\n",
            "steganography. Data Outside of View. One of the simplest forms of PDF file steganography is placing\n",
            "data in areas of the file in which it will not be rendered when opened by a PDF reader. This could\n",
            "be as simple as placing hidden data within a PDF stream comment or after the %%EOF line of the file.\n",
            "More complex strategies include placing hidden data inside an object that is not linked to the main\n",
            "object tree within the PDF – for example, a page object that is not linked into the document’s page\n",
            "catalogue [4]. The main drawback to this steganography method is that it can be easily detected and\n",
            "defeated. Between-Character, Between-Word, Between-Line. Similar to other forms of text-file\n",
            "steganography, whitespace is a vector for  1.2 Our Research  We have proposed a new strategy for\n",
            "embedding sensitive data into PDF documents through imperceptible modifications to the PDF stream\n",
            "operator values. Compared to existing PDF steganography techniques, ours can achieve a higher\n",
            "carrying capacity and embedding rate, while minimizing the visual impacts to the PDF document.\n",
            "Mostly importantly, our work looks through all the possible operators defined in the Adobe PDF\n",
            "standard, identifying all those unique operators which may be utilized to hide sensitive information\n",
            "in a steganographic manner.  2 Background Information  2.1 Structure of a PDF File  A PDF file can\n",
            "be thought of as a linked tree of objects, each of which is responsible for one piece of the\n",
            "document. For example, a two-page document will have a root object and two page parent objects, each\n",
            "of which is connected to multiple text, image, etc. objects that render the actual visual parts of\n",
            "the page.<|eot_id|><|start_header_id|>user<|end_header_id|>  How can one cryptographically hide\n",
            "malwares in a PDF document?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I don't have\n",
            "enough information to provide a specific answer. However, based on the provided context, it seems\n",
            "that the authors of the paper propose a novel PDF steganography method that utilizes an adaptable\n",
            "strategy of least-significant bit insertion on string representations of floating-point operands to\n",
            "perform imperceptible changes to PDF stream operators. They claim that their method has a higher\n",
            "carrying capacity than previous PDF operator-based methods and demonstrate its effectiveness by\n",
            "embedding malware into a given PDF cover file.  To cryptographically hide malware in a PDF document,\n",
            "one might consider using a combination of techniques such as:  1. Steganography: As mentioned in the\n",
            "paper, modifying the PDF stream operators to hide the malware. 2. Encryption: Encrypting the malware\n",
            "using a strong encryption algorithm, such as AES, before embedding it in the PDF document. 3.\n",
            "Obfuscation: Making the malware code difficult to understand or analyze by using obfuscation\n",
            "techniques, such as code compression, encryption, or anti-debugging measures. 4. Digital signatures:\n",
            "Using digital signatures to authenticate the integrity and authenticity of the PDF document, making\n",
            "it more difficult for attackers to modify or tamper with the embedded malware.  However, please note\n",
            "that hiding malware in a PDF document without proper authorization is illegal and unethical. It's\n",
            "essential to ensure that any actions taken comply with applicable laws and regulations.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(question_2)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-kuIiWuOKw4",
        "outputId": "ca9e7b27-6a67-40b5-d235-9000b94dc5e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If there is no relevant information in the\n",
            "context, just say I don't have enough information. Don't try to make up an answer.  Figure 1: The\n",
            "overall user authentication process of the proposed system.  3  KDPrint: Passive Authentication\n",
            "using Keystroke Dynamics-to-Image Encoding via Standardization  3.1 System overview  10  KDPrint:\n",
            "Passive Authentication using Keystroke Dynamics-to-Image Encoding via Standardization  6 Conclusion\n",
            "5  (1)  KDPrint: Passive Authentication using Keystroke Dynamics-to-Image Encoding via\n",
            "Standardization  indicates the size of the marker (*) at that position. Color represents the color\n",
            "of the marker, with three channels for red, green, and blue. Lastly, Opacity indicates the\n",
            "transparency of the marker. Due to the standardization of data, in the ideal user input, all\n",
            "keystrokes converge to the origin, resulting in a stabilizing effect. Consequently, the image of a\n",
            "legitimate user tends to exhibit greater stability compared to that of an impostor, as illustrated\n",
            "in Fig. 5.  Figure 3: The example of feature-engineered dataset table and image generated using our\n",
            "proposed method.  Figure 4: The keystroke dynamics to image encoding result with different scale.\n",
            "The proposed method for image generation involves representing all keystroke information on a single\n",
            "keypad button. The essential data types required for generating these images encompass Position,\n",
            "Marker Size, Color, and Opacity. Position signifies the touch location within the key and is\n",
            "represented using standardized (x, y) data. Marker size  5  (1)  KDPrint: Passive Authentication\n",
            "using Keystroke Dynamics-to-Image Encoding via\n",
            "Standardization<|eot_id|><|start_header_id|>user<|end_header_id|>  What is the benefit of encoding\n",
            "keystokes as images for training keystroke authentication mechanism, as proposed in a paper\n",
            "published in 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> According to the provided\n",
            "text, one benefit of encoding keystrokes as images is that it allows for a \"stabilizing effect\" due\n",
            "to the standardization of data. This means that the images of legitimate users tend to exhibit\n",
            "greater stability compared to those of impostors.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(question_3)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRmu3y28OLPg",
        "outputId": "8ab378c0-dfbe-4fb4-e65e-0b63b4e52a67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='7\\n\\n8\\n\\nShuaishuai Liu and Gergely Bicz´ok', metadata={'source': '/content/document1.pdf'}), -0.13994412580021032), (Document(page_content='Transparency. Inspired by the GDPR and defined eloquently by Kamleitner’s 3R insight framework [21], the sharing party (i.e., the amateur controller) can take three steps to reduce interdependent privacy risks: realize that there is a data transfer, recognize others’ rights and respect others’ rights. It is clear that transparency-enhancing technologies can facilitate the first two steps. A poten- tial way to make the sharer aware of interdependent privacy is to add a special warning sign to the already existing permission notification dialogues [26]. Such a solution has to be platform-specific and needs the cooperation of the platform owner. If such cooperation is unlikely, a dedicated interdependent privacy dash- board app can be implemented in the manner of proposed dashboard designs for Facebook apps [2]. Note that an exact public mapping of API-level per- missions to user warnings could also improve awareness (especially for browser extensions).', metadata={'source': '/content/document1.pdf'}), -0.23706774497719962), (Document(page_content='Marsch et al. proposed a design for an interdependent privacy feedback sys- tem that leverages natural language generation to communicate the potential privacy impacts of app permissions regarding both the user and others. The authors concluded that increasing awareness about the interdependent privacy consequences of app permissions could lead to more responsible privacy behavior and a more privacy-friendly app ecosystem [23]. In fact, a well-designed privacy dashboard has proved to be an efficient and user-friendly transparency-enhancing tool (TET) [24]. A privacy dashboard is a feature in apps that allows users to manage their personal data and privacy settings. Such a dashboard has also been proposed in the context of Facebook apps, specifically for raising aware- ness on IDP [2]. In recent years, big tech companies have made a push towards more transparent data practices and providing users with greater control over their personal data10 11, even implementing privacy shortcuts12, allowing users to', metadata={'source': '/content/document1.pdf'}), -0.2390583406323692), (Document(page_content='[22] N. I. of Standards and T. U. T. Administration, Secure hash standard. US Department of Commerce, Technology Administration, National Institute of . . . , 1993, vol. 180, no. 1.\\n\\n[23] “National Institute of Standards and Technology: FIPS PUB 180- 4: Secure Hash Standard. Federal Information Processing Standards Publication 180-4, U.S. Department of Commerce (March 2012),” http://csrc.nist.gov/publications/fips/fips180-4/fips-180-4.pdf.\\n\\n[24] M. Bellare and P. Rogaway, “Random oracles are practical: A paradigm the 1st ACM\\n\\nfor designing efficient protocols,” in Proceedings of Conference on Comp. and Comm. Security, 1993, pp. 62–73.\\n\\n[25] S. Goldwasser and M. Bellare, “Lecture notes on cryptography,” Summer course “Crypt. and comp. security” at MIT, vol. 1999, p. 1999, 1996. [26] C. E. Shannon, “A mathematical theory of communication,” The Bell\\n\\nsystem technical journal, vol. 27, no. 3, pp. 379–423, 1948.\\n\\n[27] Wikipedia contributors, “Eb/N0 — Wikipedia, the free encyclopedia,” [Online]. Available: https:', metadata={'source': '/content/document5.pdf'}), -0.24467946581826716)]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If there is no relevant information in the\n",
            "context, just say I don't have enough information. Don't try to make up an answer.\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>  Where is the name of the U.S.\n",
            "President?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I don't have enough information.\n"
          ]
        }
      ],
      "source": [
        "result = Chain_pdf.invoke(irrelevant_question)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "206fd266fbf546439d703c34899f9035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21734d4bb29d4ebd823561b9612d41ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30dff059815140e3afe64c656f450070": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d8eeac44145445aadaeb18c817816e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4def2765a4444f5af7e6bde16afd182",
              "IPY_MODEL_717fd309e11d4cabbf5bed389caebc76",
              "IPY_MODEL_d6dd0b5757014f88ac54061bb41f53ac"
            ],
            "layout": "IPY_MODEL_30dff059815140e3afe64c656f450070"
          }
        },
        "5faea4b926f2417f9652662de59e4cbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717fd309e11d4cabbf5bed389caebc76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c22ed6f46c4819be223a5843b93485",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_206fd266fbf546439d703c34899f9035",
            "value": 4
          }
        },
        "a4def2765a4444f5af7e6bde16afd182": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c149c72550864e3c8151b7b77466e17b",
            "placeholder": "​",
            "style": "IPY_MODEL_21734d4bb29d4ebd823561b9612d41ad",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c149c72550864e3c8151b7b77466e17b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6dd0b5757014f88ac54061bb41f53ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5faea4b926f2417f9652662de59e4cbb",
            "placeholder": "​",
            "style": "IPY_MODEL_ed0c4a982cf54ac3adc2abf9efb689e5",
            "value": " 4/4 [01:11&lt;00:00, 15.57s/it]"
          }
        },
        "ed0c4a982cf54ac3adc2abf9efb689e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4c22ed6f46c4819be223a5843b93485": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
